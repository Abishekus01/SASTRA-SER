# ğŸ§ SwinTSER â€“ Bilingual Speech Emotion Recognition  
### Using Shifted Window Transformer (Swin Transformer)

## ğŸ“Œ Project Overview
This project implements a **Speech Emotion Recognition (SER)** system that predicts human emotions from speech audio.  
It is inspired by the research paper:

> **â€œSwinTSER: An Improved Bilingual Speech Emotion Recognition Using Shift Window Transformerâ€**

The system supports **bilingual audio (English + Tamil)** and uses a **deep learning Swin Transformer model** to classify emotions from audio features.

---

## ğŸ¯ Objectives
- To recognize emotions from speech audio automatically
- To support **bilingual speech inputs**
- To apply **Transformer-based deep learning** instead of traditional CNN/RNN models (used MFCC)
- To provide a **web-based interface** for easy interaction

---

## ğŸ§  Key Idea (In Simple Terms)
1. Audio is uploaded via a web page  
2. Audio is converted into **Mel-Spectrograms** (audio â†’ image-like representation)  
3. A **Swin Transformer** model learns emotion-related patterns  
4. The predicted emotion is displayed on the web interface  

---

## ğŸ­ Supported Emotions
Example emotion classes:
- Happy  
- Sad  
- Angry  
- Neutral  
- Fear  
- Disgust  
- Surprise  

(Exact labels can be configured in `utils/config.py`)

---

## ğŸ›  Tech Stack

### Programming & Frameworks
- **Python 3.9+**
- **Flask** â€“ Backend Web Framework
- **HTML / CSS / JavaScript** â€“ Frontend

### Machine Learning & Audio
- **PyTorch** â€“ Deep Learning Framework
- **Swin Transformer** â€“ Core model
- **Librosa** â€“ Audio processing
- **NumPy, SciPy** â€“ Numerical operations

---

